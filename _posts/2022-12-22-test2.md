<h1>YeongJin</h1>


```python
from jamo import j2h
from jamo import j2hcj
from itertools import product

def Assembling(data) :
    raw_list = []
    return_list = []
    for raw in data:
        raw_list.append(raw[0])
    
    for i in range(len(raw_list)):
        sel = 0
        for j in range(3,len(raw_list[i])+3,3):
            tmp = raw_list[i][sel:j]
            sel = j

            if tmp[2] == ' ':
                return_list.append(j2h(tmp[0], tmp[1]))
                
            else :
                return_list.append(j2h(tmp[0], tmp[1], tmp[2]))
        return_list.append(' ')
    return_list = ''.join(return_list)
    return return_list
```


```python
class YJ_Stemmer:
    def __init__(self, data):#data = after POS tagging data
        self.data = data
    
    def Korean_jamo(self, data):#divide by Consonant & vowel(tokenize for korean)
        data = self.data

        CHOSUNG_LIST = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ',
                        'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']

        JUNGSUNG_LIST = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ',
                         'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']

        JONGSUNG_LIST = [' ', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ',
                         'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ',
                         'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']
        r_lst = []
        list_num=0
        
        for word in data:
            word_list = []
            for i in range(len(word[0])):
                w = word[0][i]
                if '가'<=w<='힣':
                    ch1 = (ord(w) - ord('가'))//588
                    ch2 = ((ord(w) - ord('가')) - (588*ch1)) // 28
                    ch3 = (ord(w) - ord('가')) - (588*ch1) - 28*ch2
#                     word_list.append([CHOSUNG_LIST[ch1], JUNGSUNG_LIST[ch2], JONGSUNG_LIST[ch3]])
                    word_list.append(CHOSUNG_LIST[ch1])
                    word_list.append(JUNGSUNG_LIST[ch2])
                    word_list.append(JONGSUNG_LIST[ch3])
            data[list_num] = list(data[list_num])
            data[list_num][0] = word_list
#             data[list_num] = tuple(data[list_num])
            
            list_num+=1
                
        return data

    def Stem_1Step(self):#first remove JOSA (after tokenize + 의)
        print('First it will remove JOSA')
        data = self.Korean_jamo(self.data)
        first_data = []
        for i in range(len(data)):
            if data[i][1] != 'Josa':
                first_data.append(data[i])
                
        data_num = 0
        for data_w in first_data:
            if data_w[0][-3:] == ['ㅇ','ㅢ',' ']:
                if len(data_w[0]) == 3:
                    del first_data[data_num]
                else:
                    first_data[data_num][0] = data_w[0][:-3]
                
            data_num +=1
        return first_data
    
    def Stem_2Step(self):#second remove past (ㅆ던, ㅆ다)
        print('Second it will remove past words')
        second_data = self.Stem_1Step()
        data_num = 0
        for data_w in second_data:
            if data_w[0][-4:] == ['ㅆ','ㄷ','ㅓ','ㄴ']:
                second_data[data_num][0] = data_w[0][:-9]
            
            elif data_w[0][-4:] == ['ㅆ','ㄷ','ㅏ',' ']:
                second_data[data_num][0] = data_w[0][:-9]
                
            data_num +=1
        return second_data
    
    def Stem_3Step(self):#third remove plural (들)
        print('Third it will remove plural')
        third_data = self.Stem_2Step()
        data_num = 0
        for data_w in third_data:
            if data_w[0][-3:] == ['ㄷ','ㅡ','ㄹ']:
                third_data[data_num][0] = data_w[0][:-3]
            elif data_w[0][-3:] == ['ㄴ','ㅡ','ㄴ']:
                if data_w[1] == 'Noun':
                    data_w[0][-3:] = ['ㄷ','ㅏ',' ']
                else:
                    third_data[data_num][0] = data_w[0][:-3]
            data_num += 1
        return third_data
    
    def Stem_4Step(self):
        print('Fourth it will remove ~ㅣ도, ㅏ게')
        fourth_data = self.Stem_3Step()
        data_num = 0
        for data_w in fourth_data:
            if data_w[0][-5:] == ['ㅣ',' ','ㄷ','ㅗ',' ']:
                fourth_data[data_num][0] = data_w[0][:-6]
            
            elif data_w[0][-5:] == ['ㅏ',' ','ㄱ','ㅔ',' ']:
                fourth_data[data_num][0] = data_w[0][:-6]
                
            data_num += 1
        return fourth_data
    
    def Stem_5Step(self):
        print('Fifth it will change ~며,~게,~고 to ~다')
        fifth_data = self.Stem_4Step()
        data_num = 0
        for data_w in fifth_data:
            if data_w[0][-3:] == ['ㅁ','ㅕ',' ']:
                fifth_data[data_num][0][-3:] = ['ㄷ','ㅏ',' ']
            
            elif data_w[0][-3:] == ['ㄱ','ㅔ',' ']:
                if data_w[1] != 'Noun':
                    fifth_data[data_num][0][-3:] = ['ㄷ','ㅏ',' ']
                
            elif data_w[0][-3:] == ['ㄱ','ㅗ',' ']:
                fifth_data[data_num][0][-3:] = ['ㄷ','ㅏ',' ']
                
            elif data_w[0][-4:] == ['ㅆ','ㅈ','ㅛ',' ']:
                fifth_data[data_num][0][-3:] = ['ㄷ','ㅏ',' ']
                
            data_num += 1
        return fifth_data
    
    def Stem_6Step(self):
        print('sixth it will change ~은-it is Adjective')
        sixth_data = self.Stem_5Step()
        data_num = 0
        for data_w in sixth_data:
            if data_w[0][-3:] == ['ㅇ','ㅡ','ㄴ']:
                if data_w[1] == 'Adjective':
                    sixth_data[data_num][0][-3:] = ['ㄷ','ㅏ',' ']
                
            data_num += 1
        return sixth_data
```


```python
import konlpy
import os
import glob
import numpy as np
```


```python
data_path = '../news'
data_list = glob.glob(data_path+'/*.txt')
```


```python
with open(data_list[0], "r", encoding='utf-8') as f:
        ori_data = f.read()
        
# ori_data = ori_data.split("\n")
```


```python
# print(len(ori_data))
print(len(ori_data))
# data_len = int(len(ori_data)/4)
data_len = 100000
```

    408604611
    


```python
#The corpus was too large to create morphemes.
#So, the corpus was divided and used.
data1 = ''
# data2 = ''
# data3 = ''
# data4 = ''
for i in range(data_len):
    if ori_data[i] != None:
        data1+=ori_data[i]
        
# for i in range(data_len, data_len*2):
#     if ori_data[i] != None:
#         data2+=ori_data[i]
        
# for i in range(data_len*2, data_len*3):
#     if ori_data[i] != None:
#         data3+=ori_data[i]
        
# for i in range(data_len*3, data_len*4):
#     if ori_data[i] != None:
#         data4+=ori_data[i]
        
# data_list = [data1, data2, data3, data4]
# del data1, data2, data3, data4
```


```python
# Josa_list = ['JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ', 'JX', 'JC']
```


```python
import re
# remove_punctuation_list = []
# for data in data_list:
#     remove_punctuation = re.sub(r'[^\w\s%가-힣]', ' ', data)
#     remove_punctuation = re.sub("\n", " ", remove_punctuation)
#     remove_punctuation_list.append(re.sub("\n", " ", remove_punctuation))
remove_punctuation = re.sub(r'[^\w\s%가-힣]', ' ', data1)
remove_punctuation = re.sub("\n", " ", remove_punctuation)
```


```python
remove_punctuation[:1000]
```




    '뒤이어 자유한국당  바른정당 순이었고  국민의당과 정의당이 각각 4%  없음 의견유보 28%로 조사됐다  다만 포럼을 중심으로 신당 창당도 가능하냐는 질문엔  보수우파를 분열케 하는 행동은 바람직하지 않다 고 일축했다  추미애 더불어민주당 대표는  다른 건 다 좋아요  그런데 제헌절에 유감이 있다는 대표는 뭐예요  라고 물었다  그는  지역 유권자들이 자유한국당은 지구 상에 이런 야당은 유일할 거다  희귀하고 기괴한 야당이라고 한다 며  어떤 나라의 야당도 민족의 이익이나 국가의 이익과 결부된 사안에 대해서는 힘을 모으지 않나  자유한국당은 오로지 지방선거에서 어떻게 돌파구를 찾을 것인가 여기에만 관심이 있다 고 지적했다  이 위원장은 최우선 과제로 청년 취업난 해소를 꼽았다  다만 야당이 중재안을 받아들이는 걸 전제로 시간을 더 달라고 하면 하루 이틀 정부 이송을 늦출 순 있다고 한다  추 대표는 정치권 일각에서  문 전 대표를 이미 대선의 야권 후보로 본다 는 주장에   김대중 전 대통령을 만나 입당원서를 쓰면서 정대철 전 고문에게 들은  정치는 생물 이란 말이 생생하다 며  경선도 안 했는데 확정이 어딨느냐 고 답했다  그러면서  이 정부가 남북 평화쇼를 하고 있지만  국민 불안은 가중되고 있다 고 덧붙였다  박 전 대통령은 검정색 에쿠스 차량에서 내리며 마중나온 임원주 사무국장에게 고개숙여 인사했다  정우택 의원 측은 이러한 조처가 아동학대 조기발견과 예방에 효과가 있을 것이라고 예상하고 있다  새누리당 정진석 원내대표와 김희옥 비대위원장이 7일 오전 서울 여의도 새누리당 당사에서 열린 혁신비상대책회의에 참석해 이야기를 나누고 있다  상왕십리 열차 추돌 사고 이후 시민 안전을 위해 기준이 강화됐는데  전동차 제작 과정을 점검할 필요가 있다는 것이다  당시에는 중동호흡기증후군 확산으로 혼란이 가중되던 시기여서 최 의원의 출장이 논란을 빚은 바 있다  선관위 관계자는  사법당국과 공조해 유포자를 특정한 뒤 고의성 여부를 확인할 계획 이라며  특정 후보를 낙'




```python
# from konlpy.tag import Twitter
# twit = Twitter()
# data_morphs = twit.morphs(remove_punctuation)
```


```python
from konlpy.tag import Okt
okt = Okt()
data_morphs1 = okt.pos(remove_punctuation)
# data_morphs2 = okt.pos(remove_punctuation_list[1])
# data_morphs3 = okt.pos(remove_punctuation_list[2])
# data_morphs4 = okt.pos(remove_punctuation_list[3])
```


```python
# data_morphs = mecab.morphs(remove_punctuation)
```


```python
# data_morphs
```


```python
# from sklearn.feature_extraction.text import CountVectorizer
# vector = CountVectorizer()
# bow = vector.fit_transform(data_morphs)
```


```python
def to_ngrams(words, n):
    ngrams = []
    for b in range(0, len(words) - n + 1):
        ngrams.append(tuple(words[b:b+n]))
    return ngrams
```


```python
bigram = to_ngrams(data_morphs1, 2)
sentences = []
pos_freq = []#i wanna calculate because most of Josa is followed by a noun, or 
             #there is a high probability that a proper noun is followed by noun or verb.

for gram in bigram:
    sentences.append((gram[0][0], gram[1][0]))
    
for gram in bigram:
    pos_freq.append((gram[0][1], gram[1][1]))
```


```python
from nltk import ConditionalFreqDist
from nltk.probability import ConditionalProbDist, MLEProbDist
cfd = ConditionalFreqDist(sentences)
cpd = ConditionalProbDist(cfd, MLEProbDist)

pfd = ConditionalFreqDist(pos_freq)
ppd = ConditionalProbDist(pfd, MLEProbDist)
```


```python
pfd['Josa'].most_common(7)
```




    [('Noun', 5651),
     ('Verb', 1552),
     ('Adjective', 608),
     ('Number', 323),
     ('Adverb', 134),
     ('Modifier', 123),
     ('Determiner', 93)]




```python
cfd["방안"].most_common(7)
```




    [('을', 10), ('에', 4), ('과', 2), ('등', 2), ('이라고', 1), ('문건', 1)]




```python
cpd["방안"].prob("을")
```




    0.5




```python
from konlpy.tag import Mecab
mecab = Mecab()
```


```python
youtube_data_path = '../youtube/data/Suka'
youtube_list = glob.glob(youtube_data_path+'/*.txt')

youtube_data = []
for path in youtube_list:
    with open(path, "r", encoding='utf-8') as f:
        youtube_data.append(f.read())
```


```python
y_remove_punctuation = re.sub(r'[^\w\s%가-힣]', ' ', youtube_data[0])
y_remove_punctuation = re.sub("\n", " ", y_remove_punctuation)
```


```python
youtube_morphs = okt.pos(y_remove_punctuation)
```


```python
Stemer = YJ_Stemmer(youtube_morphs)
youtube_stem = Stemer.Stem_6Step()
```

    sixth it will change ~은-it is Adjective
    Fifth it will change ~며,~게,~고 to ~다
    Fourth it will remove ~ㅣ도, ㅏ게
    Third it will remove plural
    Second it will remove past words
    First it will remove JOSA
    


```python
print(youtube_morphs[:100])
```

    [[['ㅈ', 'ㅏ', ' '], 'Noun'], [['ㅇ', 'ㅣ', ' ', 'ㅅ', 'ㅏ', 'ㅇ', 'ㅎ', 'ㅏ', 'ㄴ'], 'Adjective'], [['ㅂ', 'ㅕ', 'ㄴ', 'ㅎ', 'ㅗ', ' ', 'ㅅ', 'ㅏ', ' '], 'Noun'], [['ㅇ', 'ㅜ', ' ', 'ㅇ', 'ㅕ', 'ㅇ'], 'Noun'], [['ㅇ', 'ㅜ', ' '], 'Noun'], [['ㅅ', 'ㅣ', 'ㄴ', 'ㄷ', 'ㅡ', ' ', 'ㄹ', 'ㅗ', 'ㅁ'], 'Noun'], [['ㅇ', 'ㅣ', ' '], 'Josa'], [['ㅂ', 'ㅜ', 'ㄹ'], 'Noun'], [['ㄱ', 'ㅗ', ' '], 'Josa'], [['ㅇ', 'ㅣ', 'ㅆ', 'ㄷ', 'ㅏ', ' '], 'Adjective'], [['ㄸ', 'ㅗ', 'ㄱ', 'ㅂ', 'ㅏ', ' ', 'ㄹ', 'ㅗ', ' '], 'Noun'], [['ㅇ', 'ㅣ', 'ㄺ', 'ㅇ', 'ㅓ', ' ', 'ㄷ', 'ㅗ', ' '], 'Verb'], [['ㄱ', 'ㅓ', ' ', 'ㄲ', 'ㅜ', ' ', 'ㄹ', 'ㅗ', ' '], 'Noun'], [['ㅇ', 'ㅣ', 'ㄺ', 'ㅇ', 'ㅓ', ' ', 'ㄷ', 'ㅗ', ' '], 'Verb'], [['ㅇ', 'ㅜ', ' ', 'ㅇ', 'ㅕ', 'ㅇ'], 'Noun'], [['ㅇ', 'ㅜ', ' '], 'Adverb'], [['ㄹ', 'ㅏ', ' ', 'ㄱ', 'ㅗ', ' '], 'Josa'], [['ㄱ', 'ㅘ', 'ㅇ', 'ㄷ', 'ㅏ', ' '], 'Noun'], [['ㄹ', 'ㅡ', 'ㄹ'], 'Josa'], [['ㅎ', 'ㅏ', ' '], 'Verb'], [['ㅇ', 'ㅣ', ' ', 'ㅅ', 'ㅏ', 'ㅇ', 'ㅎ', 'ㅏ', 'ㄴ'], 'Adjective'], [['ㅂ', 'ㅕ', 'ㄴ', 'ㅎ', 'ㅗ', ' ', 'ㅅ', 'ㅏ', ' '], 'Noun'], [['ㅇ', 'ㅜ', ' ', 'ㅇ', 'ㅕ', 'ㅇ'], 'Noun'], [['ㅇ', 'ㅜ', ' '], 'Adverb'], [['ㄱ', 'ㅏ', ' '], 'Verb'], [['ㄷ', 'ㅚ', ' ', 'ㄱ', 'ㅔ', 'ㅆ', 'ㅅ', 'ㅡ', 'ㅂ', 'ㄴ', 'ㅣ', ' ', 'ㄷ', 'ㅏ', ' '], 'Verb'], [['ㅈ', 'ㅏ', ' '], 'Noun'], [['ㅂ', 'ㅗ', ' ', 'ㅅ', 'ㅣ', 'ㄴ'], 'Noun'], [['ㅂ', 'ㅜ', 'ㄴ'], 'Noun'], [['ㅇ', 'ㅏ', 'ㄹ', 'ㄱ', 'ㅔ', 'ㅆ', 'ㅈ', 'ㅣ', ' ', 'ㅁ', 'ㅏ', 'ㄴ'], 'Verb'], [[], 'Alpha'], [[], 'Number'], [['ㅇ', 'ㅔ', ' '], 'Josa'], [['ㅁ', 'ㅣ', ' ', 'ㅊ', 'ㅣ', 'ㄹ'], 'Adjective'], [['ㄷ', 'ㅡ', 'ㅅ'], 'Noun'], [['ㅎ', 'ㅏ', 'ㄴ'], 'Josa'], [['ㅊ', 'ㅓ', 'ㄴ', 'ㅈ', 'ㅐ', ' '], 'Noun'], [['ㅅ', 'ㅓ', ' ', 'ㅇ', 'ㅜ', 'ㄹ', 'ㄷ', 'ㅐ', ' '], 'Noun'], [['ㄹ', 'ㅗ', ' ', 'ㅅ', 'ㅡ', ' ', 'ㅋ', 'ㅜ', 'ㄹ'], 'Noun'], [['ㅅ', 'ㅜ', ' ', 'ㅅ', 'ㅓ', 'ㄱ'], 'Noun'], [['ㅈ', 'ㅗ', 'ㄹ', 'ㅇ', 'ㅓ', 'ㅂ'], 'Noun'], [['ㅇ', 'ㅣ', 'ㄴ', 'ㄱ', 'ㅏ', ' '], 'Josa'], [['ㄱ', 'ㅡ', ' ', 'ㄹ', 'ㅓ', 'ㅎ', 'ㅈ', 'ㅛ', ' '], 'Adjective'], [['ㄱ', 'ㅡ', ' ', 'ㄹ', 'ㅓ', 'ㄴ'], 'Adjective'], [['ㅇ', 'ㅓ', 'ㅁ', 'ㅊ', 'ㅓ', 'ㅇ', 'ㄴ', 'ㅏ', 'ㄴ'], 'Adjective'], [['ㅊ', 'ㅓ', 'ㄴ', 'ㅈ', 'ㅐ', ' '], 'Noun'], [['ㅈ', 'ㅓ', 'ㄱ'], 'Suffix'], [['ㅇ', 'ㅣ', 'ㄴ'], 'Josa'], [['ㅂ', 'ㅕ', 'ㄴ', 'ㅎ', 'ㅗ', ' ', 'ㅅ', 'ㅏ', ' '], 'Noun'], [['ㅇ', 'ㅣ', 'ㄴ', 'ㄷ', 'ㅔ', ' '], 'Josa'], [['ㄱ', 'ㅚ', 'ㅇ', 'ㅈ', 'ㅏ', 'ㅇ', 'ㅎ', 'ㅣ', ' '], 'Adjective'], [['ㅅ', 'ㅜ', 'ㄴ', 'ㅅ', 'ㅜ', ' ', 'ㅎ', 'ㅏ', ' ', 'ㄷ', 'ㅏ', ' '], 'Adjective'], [['ㅈ', 'ㅓ', 'ㅇ'], 'Noun'], [['ㄹ', 'ㅗ', 'ㅂ', 'ㄱ', 'ㅗ', ' '], 'Josa'], [['ㅅ', 'ㅏ', ' ', 'ㄹ', 'ㅏ', 'ㅇ', 'ㅅ', 'ㅡ', ' ', 'ㄹ', 'ㅓ', ' ', 'ㅇ', 'ㅜ', 'ㄴ'], 'Adjective'], [['ㅋ', 'ㅐ', ' ', 'ㄹ', 'ㅣ', 'ㄱ', 'ㅌ', 'ㅓ', ' '], 'Noun'], [['ㄹ', 'ㅗ', ' '], 'Josa'], [['ㄴ', 'ㅏ', ' ', 'ㅇ', 'ㅗ', ' ', 'ㄴ', 'ㅡ', 'ㄴ', 'ㄷ', 'ㅔ', ' '], 'Verb'], [['ㄷ', 'ㅜ', ' '], 'Noun'], [['ㄱ', 'ㅐ', ' '], 'Noun'], [['ㄱ', 'ㅏ', ' '], 'Josa'], [['ㅅ', 'ㅏ', ' ', 'ㅅ', 'ㅣ', 'ㄹ'], 'Noun'], [['ㅇ', 'ㅡ', 'ㄴ'], 'Josa'], [['ㄱ', 'ㅏ', 'ㅌ', 'ㅇ', 'ㅣ', ' '], 'Adverb'], [['ㅇ', 'ㅓ', 'ㄺ', 'ㄱ', 'ㅣ', ' ', 'ㄱ', 'ㅏ', ' '], 'Verb'], [['ㅅ', 'ㅟ', 'ㅂ', 'ㅈ', 'ㅣ', ' '], 'Verb'], [['ㅇ', 'ㅏ', 'ㄶ', 'ㅈ', 'ㅏ', 'ㄶ', 'ㅇ', 'ㅏ', ' ', 'ㅇ', 'ㅛ', ' '], 'Verb'], [['ㄱ', 'ㅚ', 'ㅇ', 'ㅈ', 'ㅏ', 'ㅇ', 'ㅎ', 'ㅣ', ' '], 'Adjective'], [['ㅊ', 'ㅓ', 'ㄴ', 'ㅈ', 'ㅐ', ' '], 'Noun'], [['ㅇ', 'ㅣ', 'ㄴ', 'ㄷ', 'ㅔ', ' '], 'Josa'], [['ㄱ', 'ㅚ', 'ㅇ', 'ㅈ', 'ㅏ', 'ㅇ', 'ㅎ', 'ㅣ', ' '], 'Adjective'], [['ㅅ', 'ㅜ', 'ㄴ', 'ㅅ', 'ㅜ', ' ', 'ㅎ', 'ㅏ', ' ', 'ㄷ', 'ㅏ', ' '], 'Adjective'], [['ㅅ', 'ㅏ', ' ', 'ㄹ', 'ㅏ', 'ㅇ', 'ㅅ', 'ㅡ', ' ', 'ㄹ', 'ㅓ', ' ', 'ㅇ', 'ㅝ', ' '], 'Adjective'], [['ㅇ', 'ㅣ', ' '], 'Determiner'], [['ㄱ', 'ㅓ', ' '], 'Noun'], [['ㄹ', 'ㅡ', 'ㄹ'], 'Josa'], [['ㅅ', 'ㅏ', ' ', 'ㄹ', 'ㅏ', 'ㅁ'], 'Noun'], [[], 'Suffix'], [['ㅇ', 'ㅣ', ' '], 'Josa'], [['ㅅ', 'ㅏ', ' ', 'ㅅ', 'ㅣ', 'ㄹ'], 'Noun'], [['ㅇ', 'ㅡ', 'ㄴ'], 'Josa'], [['ㅈ', 'ㅏ', 'ㄹ'], 'Verb'], [['ㅂ', 'ㅏ', ' ', 'ㄹ', 'ㅏ', ' ', 'ㅈ', 'ㅣ', ' '], 'Noun'], [['ㅇ', 'ㅏ', 'ㄶ', 'ㅇ', 'ㅏ', ' ', 'ㅇ', 'ㅛ', ' '], 'Verb'], [['ㅈ', 'ㅐ', ' ', 'ㅂ', 'ㅓ', 'ㄹ'], 'Noun'], [['ㅇ', 'ㅣ', ' ', 'ㅇ', 'ㅑ', ' '], 'Josa'], [['ㅈ', 'ㅓ', 'ㅇ', 'ㅁ', 'ㅏ', 'ㄹ'], 'Noun'], [['ㅈ', 'ㅗ', ' '], 'Number'], [['ㄱ', 'ㅏ', ' '], 'Foreign'], [['ㅇ', 'ㅣ', 'ㅆ', 'ㅇ', 'ㅓ', ' '], 'Adjective'], [['ㅇ', 'ㅏ', ' ', 'ㅃ', 'ㅏ', ' '], 'Noun'], [['ㄹ', 'ㅗ', ' ', 'ㅂ', 'ㅜ', ' ', 'ㅌ', 'ㅓ', ' '], 'Noun'], [['ㅁ', 'ㅜ', 'ㄹ', 'ㄹ', 'ㅕ', ' '], 'Verb'], [['ㅂ', 'ㅏ', 'ㄷ', 'ㅇ', 'ㅏ', 'ㅆ', 'ㅇ', 'ㅓ', ' '], 'Verb'], [['ㄱ', 'ㅡ', 'ㄴ', 'ㄷ', 'ㅔ', ' '], 'Adverb'], [['ㅈ', 'ㅓ', 'ㅇ'], 'Noun'], [['ㄹ', 'ㅗ', 'ㅂ', 'ㄱ', 'ㅗ', ' '], 'Josa'], [['ㅅ', 'ㅜ', 'ㄴ', 'ㅅ', 'ㅜ', ' ', 'ㅎ', 'ㅏ', ' ', 'ㄷ', 'ㅏ', ' '], 'Adjective'], [['ㅅ', 'ㅏ', ' ', 'ㄹ', 'ㅏ', 'ㅇ', 'ㅅ', 'ㅡ', ' ', 'ㄹ', 'ㅓ', ' ', 'ㅇ', 'ㅝ', ' '], 'Adjective'], [['ㄱ', 'ㅡ', ' ', 'ㄸ', 'ㅏ', 'ㄴ'], 'Modifier']]
    


```python
assem_kor = Assembling(youtube_morphs)
print(assem_kor[:100])
```

    자 이상한 변호사 우영 우 신드롬 이 불 고 있다 똑바로 읽어도 거꾸로 읽어도 우영 우 라고 광다 를 하 이상한 변호사 우영 우 가 되겠습니다 자 보신 분 알겠지만   에 미칠 듯
    


```python
assem_kor = mecab.morphs(assem_kor)#tokenizer
```


```python
new_sentence = []
for w in assem_kor: 
    if w != ' ':
        new_w = cfd[w].most_common(1)
        new_sentence.append(w)
        if new_w != []:
            new_sentence.append(new_w[0][0])
```


```python
#A very funny result came out. (not mean good!)
#I haven't used all the data I have, but if you use more data with add Trigram the results will be better! Thank you
```
