---
layout: post
title:  NLP_Stemming
date:   2022-12-22 16:40:16
description: march & april, looking forward to summer
tags: formatting links
categories: sample-posts
---

<h1>YeongJin</h1>


```python
from jamo import j2h
from jamo import j2hcj
from itertools import product

def Assembling(data) :
    raw_list = []
    return_list = []
    for raw in data:
        raw_list.append(raw[0])
    
    for i in range(len(raw_list)):
        sel = 0
        for j in range(3,len(raw_list[i])+3,3):
            tmp = raw_list[i][sel:j]
            sel = j

            if tmp[2] == ' ':
                return_list.append(j2h(tmp[0], tmp[1]))
                
            else :
                return_list.append(j2h(tmp[0], tmp[1], tmp[2]))
        return_list.append(' ')
    return_list = ''.join(return_list)
    return return_list
```


```python
class YJ_Stemmer:
    def __init__(self, data):#data = after POS tagging data
        self.data = data
    
    def Korean_jamo(self, data):#divide by Consonant & vowel(tokenize for korean)
        data = self.data

        CHOSUNG_LIST = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ',
                        'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']

        JUNGSUNG_LIST = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ',
                         'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']

        JONGSUNG_LIST = [' ', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ',
                         'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ',
                         'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']
        r_lst = []
        list_num=0
        
        for word in data:
            word_list = []
            for i in range(len(word[0])):
                w = word[0][i]
                if '가'<=w<='힣':
                    ch1 = (ord(w) - ord('가'))//588
                    ch2 = ((ord(w) - ord('가')) - (588*ch1)) // 28
                    ch3 = (ord(w) - ord('가')) - (588*ch1) - 28*ch2
#                     word_list.append([CHOSUNG_LIST[ch1], JUNGSUNG_LIST[ch2], JONGSUNG_LIST[ch3]])
                    word_list.append(CHOSUNG_LIST[ch1])
                    word_list.append(JUNGSUNG_LIST[ch2])
                    word_list.append(JONGSUNG_LIST[ch3])
            data[list_num] = list(data[list_num])
            data[list_num][0] = word_list
#             data[list_num] = tuple(data[list_num])
            
            list_num+=1
                
        return data

    def Stem_1Step(self):#first remove JOSA (after tokenize + 의)
        print('First it will remove JOSA')
        data = self.Korean_jamo(self.data)
        first_data = []
        for i in range(len(data)):
            if data[i][1] != 'Josa':
                first_data.append(data[i])
                
        data_num = 0
        for data_w in first_data:
            if data_w[0][-3:] == ['ㅇ','ㅢ',' ']:
                if len(data_w[0]) == 3:
                    del first_data[data_num]
                else:
                    first_data[data_num][0] = data_w[0][:-3]
                
            data_num +=1
        return first_data
    
    def Stem_2Step(self):#second remove past (ㅆ던, ㅆ다)
        print('Second it will remove past words')
        second_data = self.Stem_1Step()
        data_num = 0
        for data_w in second_data:
            if data_w[0][-4:] == ['ㅆ','ㄷ','ㅓ','ㄴ']:
                second_data[data_num][0] = data_w[0][:-9]
            
            elif data_w[0][-4:] == ['ㅆ','ㄷ','ㅏ',' ']:
                second_data[data_num][0] = data_w[0][:-9]
                
            data_num +=1
        return second_data
    
    def Stem_3Step(self):#third remove plural (들)
        print('Third it will remove plural')
        third_data = self.Stem_2Step()
        data_num = 0
        for data_w in third_data:
            if data_w[0][-3:] == ['ㄷ','ㅡ','ㄹ']:
                third_data[data_num][0] = data_w[0][:-3]
            elif data_w[0][-3:] == ['ㄴ','ㅡ','ㄴ']:
                if data_w[1] == 'Noun':
                    data_w[0][-3:] = ['ㄷ','ㅏ',' ']
                else:
                    third_data[data_num][0] = data_w[0][:-3]
            data_num += 1
        return third_data
    
    def Stem_4Step(self):
        print('Fourth it will remove ~ㅣ도, ㅏ게')
        fourth_data = self.Stem_3Step()
        data_num = 0
        for data_w in fourth_data:
            if data_w[0][-5:] == ['ㅣ',' ','ㄷ','ㅗ',' ']:
                fourth_data[data_num][0] = data_w[0][:-6]
            
            elif data_w[0][-5:] == ['ㅏ',' ','ㄱ','ㅔ',' ']:
                fourth_data[data_num][0] = data_w[0][:-6]
                
            data_num += 1
        return fourth_data
    
    def Stem_5Step(self):
        print('Fifth it will change ~며,~게,~고 to ~다')
        fifth_data = self.Stem_4Step()
        data_num = 0
        for data_w in fifth_data:
            if data_w[0][-3:] == ['ㅁ','ㅕ',' ']:
                fifth_data[data_num][0][-3:] = ['ㄷ','ㅏ',' ']
            
            elif data_w[0][-3:] == ['ㄱ','ㅔ',' ']:
                if data_w[1] != 'Noun':
                    fifth_data[data_num][0][-3:] = ['ㄷ','ㅏ',' ']
                
            elif data_w[0][-3:] == ['ㄱ','ㅗ',' ']:
                fifth_data[data_num][0][-3:] = ['ㄷ','ㅏ',' ']
                
            elif data_w[0][-4:] == ['ㅆ','ㅈ','ㅛ',' ']:
                fifth_data[data_num][0][-3:] = ['ㄷ','ㅏ',' ']
                
            data_num += 1
        return fifth_data
    
    def Stem_6Step(self):
        print('sixth it will change ~은-it is Adjective')
        sixth_data = self.Stem_5Step()
        data_num = 0
        for data_w in sixth_data:
            if data_w[0][-3:] == ['ㅇ','ㅡ','ㄴ']:
                if data_w[1] == 'Adjective':
                    sixth_data[data_num][0][-3:] = ['ㄷ','ㅏ',' ']
                
            data_num += 1
        return sixth_data
```


```python
import konlpy
import os
import glob
import numpy as np
```


```python
data_path = '../news'
data_list = glob.glob(data_path+'/*.txt')
```


```python
with open(data_list[0], "r", encoding='utf-8') as f:
        ori_data = f.read()
        
# ori_data = ori_data.split("\n")
```


```python
# print(len(ori_data))
print(len(ori_data))
# data_len = int(len(ori_data)/4)
data_len = 100000
```

    408604611
    


```python
#The corpus was too large to create morphemes.
#So, the corpus was divided and used.
data1 = ''
# data2 = ''
# data3 = ''
# data4 = ''
for i in range(data_len):
    if ori_data[i] != None:
        data1+=ori_data[i]
        
# for i in range(data_len, data_len*2):
#     if ori_data[i] != None:
#         data2+=ori_data[i]
        
# for i in range(data_len*2, data_len*3):
#     if ori_data[i] != None:
#         data3+=ori_data[i]
        
# for i in range(data_len*3, data_len*4):
#     if ori_data[i] != None:
#         data4+=ori_data[i]
        
# data_list = [data1, data2, data3, data4]
# del data1, data2, data3, data4
```


```python
# Josa_list = ['JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ', 'JX', 'JC']
```


```python
import re
# remove_punctuation_list = []
# for data in data_list:
#     remove_punctuation = re.sub(r'[^\w\s%가-힣]', ' ', data)
#     remove_punctuation = re.sub("\n", " ", remove_punctuation)
#     remove_punctuation_list.append(re.sub("\n", " ", remove_punctuation))
remove_punctuation = re.sub(r'[^\w\s%가-힣]', ' ', data1)
remove_punctuation = re.sub("\n", " ", remove_punctuation)
```


```python
# from konlpy.tag import Twitter
# twit = Twitter()
# data_morphs = twit.morphs(remove_punctuation)
```


```python
from konlpy.tag import Okt
okt = Okt()
data_morphs1 = okt.pos(remove_punctuation)
# data_morphs2 = okt.pos(remove_punctuation_list[1])
# data_morphs3 = okt.pos(remove_punctuation_list[2])
# data_morphs4 = okt.pos(remove_punctuation_list[3])
```


```python
# data_morphs = mecab.morphs(remove_punctuation)
```


```python
# data_morphs
```


```python
# from sklearn.feature_extraction.text import CountVectorizer
# vector = CountVectorizer()
# bow = vector.fit_transform(data_morphs)
```


```python
def to_ngrams(words, n):
    ngrams = []
    for b in range(0, len(words) - n + 1):
        ngrams.append(tuple(words[b:b+n]))
    return ngrams
```


```python
bigram = to_ngrams(data_morphs1, 2)
sentences = []
pos_freq = []#i wanna calculate because most of Josa is followed by a noun, or 
             #there is a high probability that a proper noun is followed by noun or verb.

for gram in bigram:
    sentences.append((gram[0][0], gram[1][0]))
    
for gram in bigram:
    pos_freq.append((gram[0][1], gram[1][1]))
```


```python
from nltk import ConditionalFreqDist
from nltk.probability import ConditionalProbDist, MLEProbDist
cfd = ConditionalFreqDist(sentences)
cpd = ConditionalProbDist(cfd, MLEProbDist)

pfd = ConditionalFreqDist(pos_freq)
ppd = ConditionalProbDist(pfd, MLEProbDist)
```


```python
pfd['Josa'].most_common(7)
```




    [('Noun', 5651),
     ('Verb', 1552),
     ('Adjective', 608),
     ('Number', 323),
     ('Adverb', 134),
     ('Modifier', 123),
     ('Determiner', 93)]




```python
cfd["방안"].most_common(7)
```




    [('을', 10), ('에', 4), ('과', 2), ('등', 2), ('이라고', 1), ('문건', 1)]




```python
cpd["방안"].prob("을")
```




    0.5




```python
from konlpy.tag import Mecab
mecab = Mecab()
```


```python
youtube_data_path = '../youtube/data/Suka'
youtube_list = glob.glob(youtube_data_path+'/*.txt')

youtube_data = []
for path in youtube_list:
    with open(path, "r", encoding='utf-8') as f:
        youtube_data.append(f.read())
```


```python
y_remove_punctuation = re.sub(r'[^\w\s%가-힣]', ' ', youtube_data[0])
y_remove_punctuation = re.sub("\n", " ", y_remove_punctuation)
```


```python
youtube_morphs = okt.pos(y_remove_punctuation)
```


```python
Stemer = YJ_Stemmer(youtube_morphs)
youtube_stem = Stemer.Stem_6Step()
```

    sixth it will change ~은-it is Adjective
    Fifth it will change ~며,~게,~고 to ~다
    Fourth it will remove ~ㅣ도, ㅏ게
    Third it will remove plural
    Second it will remove past words
    First it will remove JOSA
    


```python
assem_kor = mecab.morphs(assem_kor)#tokenizer
```


```python
new_sentence = []
for w in assem_kor: 
    if w != ' ':
        new_w = cfd[w].most_common(1)
        new_sentence.append(w)
        if new_w != []:
            new_sentence.append(new_w[0][0])
```


```python
#A very funny result came out. (not mean good!)
#I haven't used all the data I have, but if you use more data with add Trigram the results will be better! Thank you
```
