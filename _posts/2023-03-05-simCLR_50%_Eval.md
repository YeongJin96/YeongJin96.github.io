---
layout: post
title:  SimCLR code(Linear evaluate)
date:   2023-03-05 16:40:16
description: SimCLR 선형회귀평가 코드입니다.
tags: SimCLR
categories: experiment code
---




```python
import torch
import torchvision
import numpy as np
import argparse
import torch.nn as nn
```


```python
use_tpu = False
if use_tpu:
  VERSION = "20200220" #@param ["20200220","nightly", "xrt==1.15.0"]
  !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py
  !python pytorch-xla-env-setup.py --version $VERSION
```


```python
class LogisticRegression(nn.Module):
    def __init__(self, n_features, n_classes):
        super(LogisticRegression, self).__init__()

        self.model = nn.Linear(n_features, n_classes)

    def forward(self, x):
        return self.model(x)
```


```python
def train(args, loader, model, criterion, optimizer):
    loss_epoch = 0
    accuracy_epoch = 0
    model.train()
    for step, (x, y) in enumerate(loader):
        optimizer.zero_grad()

        x = x.to(args.device)
        y = y.to(args.device)

        output = model(x)
        loss = criterion(output, y)

        predicted = output.argmax(1)
        acc = (predicted == y).sum().item() / y.size(0)
        accuracy_epoch += acc

        loss.backward()
        optimizer.step()

        loss_epoch += loss.item()
#         if step % 100 == 0:
#             print(
#                 f"Step [{step}/{len(loader)}]\t Loss: {loss.item()}\t Accuracy: {acc}"
#             )

    return loss_epoch, accuracy_epoch
```


```python
def test(args, loader, model, criterion, optimizer):
    loss_epoch = 0
    accuracy_epoch = 0
    model.eval()
    for step, (x, y) in enumerate(loader):
        model.zero_grad()

        x = x.to(args.device)
        y = y.to(args.device)

        output = model(x)
        loss = criterion(output, y)

        predicted = output.argmax(1)
        acc = (predicted == y).sum().item() / y.size(0)
        accuracy_epoch += acc

        loss_epoch += loss.item()
        
    return loss_epoch, accuracy_epoch
```


```python
class TransformsSimCLR:
    """
    A stochastic data augmentation module that transforms any given data example randomly
    resulting in two correlated views of the same example,
    denoted x ̃i and x ̃j, which we consider as a positive pair.
    """

    def __init__(self, size):
        s = 1
        color_jitter = torchvision.transforms.ColorJitter(
            0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s
        )
        self.train_transform = torchvision.transforms.Compose(
            [
                torchvision.transforms.RandomResizedCrop(size=size),
                torchvision.transforms.RandomHorizontalFlip(),  # with 0.5 probability
                torchvision.transforms.RandomApply([color_jitter], p=0.8),
                torchvision.transforms.RandomGrayscale(p=0.2),
                torchvision.transforms.ToTensor(),
            ]
        )

        self.test_transform = torchvision.transforms.Compose(
            [
                torchvision.transforms.Resize(size=size),
                torchvision.transforms.ToTensor(),
            ]
        )

    def __call__(self, x):
        return self.train_transform(x), self.train_transform(x)
```


```python
import os
import yaml

def yaml_config_hook(config_file):
    """
    Custom YAML config loader, which can include other yaml files (I like using config files
    insteaad of using argparser)
    """

    # load yaml files in the nested 'defaults' section, which include defaults for experiments
    with open(config_file) as f:
        cfg = yaml.safe_load(f) or {}
        for d in cfg.get("defaults", []):
            config_dir, cf = d.popitem()
            cf = os.path.join(os.path.dirname(config_file), config_dir, cf + ".yaml")
            with open(cf) as f:
                l = yaml.safe_load(f)
                cfg.update(l)

    if "defaults" in cfg.keys():
        del cfg["defaults"]

    return cfg
```


```python
def get_resnet(name, pretrained=False):
    resnets = {
        "resnet18": torchvision.models.resnet18(pretrained=pretrained),
        "resnet50": torchvision.models.resnet50(pretrained=pretrained),
    }
    if name not in resnets.keys():
        raise KeyError(f"{name} is not a valid ResNet version")
    return resnets[name]
```


```python
class SimCLR(nn.Module):
    """
    We opt for simplicity and adopt the commonly used ResNet (He et al., 2016) to obtain hi = f(x ̃i) = ResNet(x ̃i) where hi ∈ Rd is the output after the average pooling layer.
    """

    def __init__(self, encoder, projection_dim, n_features):
        super(SimCLR, self).__init__()

        self.encoder = encoder
        self.n_features = n_features

        # Replace the fc layer with an Identity function
        self.encoder.fc = Identity()

        # We use a MLP with one hidden layer to obtain z_i = g(h_i) = W(2)σ(W(1)h_i) where σ is a ReLU non-linearity.
        self.projector = nn.Sequential(
            nn.Linear(self.n_features, self.n_features, bias=False),
            nn.ReLU(),
            nn.Linear(self.n_features, projection_dim, bias=False),
        )

    def forward(self, x_i, x_j):
        h_i = self.encoder(x_i)
        h_j = self.encoder(x_j)

        z_i = self.projector(h_i)
        z_j = self.projector(h_j)
        return h_i, h_j, z_i, z_j
```


```python
from torch.optim.optimizer import Optimizer, required
```


```python
class Identity(nn.Module):
    def __init__(self):
        super(Identity, self).__init__()

    def forward(self, x):
        return x
```


```python
from pprint import pprint

parser = argparse.ArgumentParser(description="SimCLR")
config = yaml_config_hook("./config/config.yaml")####config folder path
for k, v in config.items():
    parser.add_argument(f"--{k}", default=v, type=type(v))

args = parser.parse_args([])

if use_tpu:
  args.device = dev
else:
  args.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
```


```python
args.dataset = "Custom" # make sure to check this with the (pre-)trained checkpoint
args.resnet = "resnet50" # make sure to check this with the (pre-)trained checkpoint
args.model_path = "./"
args.logistic_batch_size = 64
args.logistic_epochs = 100
```


```python
import copy
pre_model_dir_path = './models/50%'

def pretext_model_save(model, epoch, pre_model_dir_path=pre_model_dir_path):
    if not os.path.isdir(pre_model_dir_path):
        os.makedirs(pre_model_dir_path, exist_ok=True)
    
    torch.save(model.state_dict(), pre_model_dir_path+'/best-parameters.pt')
```


```python
from torch.utils.data import random_split
def Train_valid_split(dataset, test_split=0.2, batch_size=args.logistic_batch_size):
    dataset_size = len(dataset)
    val_size = int(test_split * dataset_size)
    train_size = dataset_size - val_size

    train_dataset, val_dataset = random_split(dataset,
                                               [train_size, val_size])
    
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=batch_size,
        drop_last=False,
        shuffle=True,
        num_workers=args.workers,)
    
    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=batch_size,
        drop_last=False,
        shuffle=False,
        num_workers=args.workers,)

    return train_loader, val_loader
```


```python
from torchvision.datasets import ImageFolder
train_dataset = ImageFolder(root='../research/data/train_%50', transform=TransformsSimCLR(size=args.image_size).test_transform)
test_dataset = ImageFolder(root='../research/data/PNU/test', transform=TransformsSimCLR(size=args.image_size).test_transform)


train_loader, val_loader = Train_valid_split(train_dataset)

test_loader = torch.utils.data.DataLoader(test_dataset, 
                                          batch_size=args.logistic_batch_size,
                                          drop_last=False,
                                          shuffle = False,)
```


```python
encoder = get_resnet(args.resnet, pretrained=False)
n_features = encoder.fc.in_features

# load pre-trained model from checkpoint
simclr_model = SimCLR(encoder, args.projection_dim, n_features)
model_fp = os.path.join(
    args.model_path, "SimCLR_50%_E1909.pt"
)
simclr_model.load_state_dict(torch.load(model_fp, map_location=args.device.type))
simclr_model = simclr_model.to(args.device)
simclr_model.eval()
```






```python
## Logistic Regression
n_classes = 3
model = LogisticRegression(simclr_model.n_features, n_classes)
model = model.to(args.device)
```


```python
optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)
criterion = torch.nn.CrossEntropyLoss()
```


```python
def inference(loader, simclr_model, device):
    feature_vector = []
    labels_vector = []
    for step, (x, y) in enumerate(loader):
        x = x.to(device)

        # get encoding
        with torch.no_grad():
            h, _, z, _ = simclr_model(x, x)

        h = h.detach()

        feature_vector.extend(h.cpu().detach().numpy())
        labels_vector.extend(y.numpy())

        if step % 20 == 0:
            print(f"Step [{step}/{len(loader)}]\t Computing features...")

    feature_vector = np.array(feature_vector)
    labels_vector = np.array(labels_vector)
    print("Features shape {}".format(feature_vector.shape))
    return feature_vector, labels_vector


def get_features(context_model, train_loader, val_loader, test_loader, device):
    train_X, train_y = inference(train_loader, context_model, device)
    val_X, val_y = inference(val_loader, context_model, device)
    test_X, test_y = inference(test_loader, context_model, device)
    return train_X, train_y, val_X, val_y, test_X, test_y


def create_data_loaders_from_arrays(X_train, y_train, X_val, y_val, X_test, y_test, batch_size):
    train = torch.utils.data.TensorDataset(
        torch.from_numpy(X_train), torch.from_numpy(y_train)
    )
    train_loader = torch.utils.data.DataLoader(
        train, batch_size=batch_size, shuffle=False
    )
    
    val = torch.utils.data.TensorDataset(
        torch.from_numpy(X_val), torch.from_numpy(y_val)
    )
    val_loader = torch.utils.data.DataLoader(
        val, batch_size=batch_size, shuffle=False
    )

    test = torch.utils.data.TensorDataset(
        torch.from_numpy(X_test), torch.from_numpy(y_test)
    )
    test_loader = torch.utils.data.DataLoader(
        test, batch_size=batch_size, shuffle=False
    )
    return train_loader, val_loader, test_loader
```


```python
print("### Creating features from pre-trained context model ###")
(train_X, train_y, val_X, val_y, test_X, test_y) = get_features(
    simclr_model, train_loader, val_loader, test_loader, args.device
)

arr_train_loader, arr_val_loader, arr_test_loader = create_data_loaders_from_arrays(
    train_X, train_y, val_X, val_y, test_X, test_y, args.logistic_batch_size
)
```

    ### Creating features from pre-trained context model ###
    Step [0/58]	 Computing features...
    Step [20/58]	 Computing features...
    Step [40/58]	 Computing features...
    Features shape (3696, 2048)
    Step [0/15]	 Computing features...
    Features shape (924, 2048)
    Step [0/60]	 Computing features...
    Step [20/60]	 Computing features...
    Step [40/60]	 Computing features...
    Features shape (3782, 2048)



```python
from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter('runs/50%')
```


```python
previous_loss = 1.0
for epoch in range(args.logistic_epochs):
    loss_epoch, accuracy_epoch = train(args, arr_train_loader, model, criterion, optimizer)
    val_loss_epoch, val_accuracy_epoch = test(args, arr_val_loader, model, criterion, optimizer)
    if epoch % 5 == 0:
        print(f"Epoch [{epoch}/{args.logistic_epochs}]\t Loss: {loss_epoch / len(train_loader)}\t Accuracy: {accuracy_epoch / len(train_loader)}")
        print(f"Valid [{epoch}/{args.logistic_epochs}]\t val_Loss: {val_loss_epoch / len(val_loader)}\t val_Accuracy: {val_accuracy_epoch / len(val_loader)}")
        
    writer.add_scalars('Loss',
                          {'train':loss_epoch/len(train_loader),
                           'val':val_loss_epoch/len(val_loader)},
                         epoch)
        
    writer.add_scalars('Accuracy',
                          {'train':accuracy_epoch/len(train_loader),
                           'val': val_accuracy_epoch/len(val_loader)},
                         epoch)
    
    if previous_loss > val_loss_epoch/len(val_loader):
            pretext_model_save(model, epoch+1)
            previous_loss = copy.deepcopy(val_loss_epoch/len(val_loader))
            pre_model_path = pre_model_dir_path+'/best-parameters.pt'
            best_epoch = copy.deepcopy(epoch)
```

    Epoch [0/100]	 Loss: 0.9889860389561489	 Accuracy: 0.6524784482758621
    Valid [0/100]	 val_Loss: 0.8869942466417948	 val_Accuracy: 0.7629464285714286
    Epoch [5/100]	 Loss: 0.5573835979247915	 Accuracy: 0.8095366379310345
    Valid [5/100]	 val_Loss: 0.5597651084264119	 val_Accuracy: 0.790625
    Epoch [10/100]	 Loss: 0.4624352013242656	 Accuracy: 0.8341415229885057
    Valid [10/100]	 val_Loss: 0.48047426144282024	 val_Accuracy: 0.812797619047619
    Epoch [15/100]	 Loss: 0.4143088573011859	 Accuracy: 0.8529992816091955
    Valid [15/100]	 val_Loss: 0.4402555445830027	 val_Accuracy: 0.825595238095238
    Epoch [20/100]	 Loss: 0.3828107177697379	 Accuracy: 0.8624281609195402
    Valid [20/100]	 val_Loss: 0.41485753655433655	 val_Accuracy: 0.8404761904761905
    Epoch [25/100]	 Loss: 0.35973222116971837	 Accuracy: 0.8698814655172413
    Valid [25/100]	 val_Loss: 0.3971954603989919	 val_Accuracy: 0.8477678571428572
    Epoch [30/100]	 Loss: 0.34167221753761684	 Accuracy: 0.8787715517241379
    Valid [30/100]	 val_Loss: 0.38419572114944456	 val_Accuracy: 0.8519345238095238
    Epoch [35/100]	 Loss: 0.32689450449984647	 Accuracy: 0.8837104885057472
    Valid [35/100]	 val_Loss: 0.3742476920286814	 val_Accuracy: 0.8561011904761905
    Epoch [40/100]	 Loss: 0.31440412021916486	 Accuracy: 0.8882902298850575
    Valid [40/100]	 val_Loss: 0.3664132316907247	 val_Accuracy: 0.8602678571428571
    Epoch [45/100]	 Loss: 0.3035866489698147	 Accuracy: 0.8924209770114943
    Valid [45/100]	 val_Loss: 0.3601061741511027	 val_Accuracy: 0.8633928571428572
    Epoch [50/100]	 Loss: 0.2940407906626833	 Accuracy: 0.8972701149425287
    Valid [50/100]	 val_Loss: 0.35494018892447154	 val_Accuracy: 0.8665178571428572
    Epoch [55/100]	 Loss: 0.2854920145252655	 Accuracy: 0.9005028735632183
    Valid [55/100]	 val_Loss: 0.3506500035524368	 val_Accuracy: 0.8686011904761906
    Epoch [60/100]	 Loss: 0.2777451703260685	 Accuracy: 0.9038254310344828
    Valid [60/100]	 val_Loss: 0.3470471531152725	 val_Accuracy: 0.8686011904761906
    Epoch [65/100]	 Loss: 0.27065687909208497	 Accuracy: 0.90625
    Valid [65/100]	 val_Loss: 0.3439940959215164	 val_Accuracy: 0.8696428571428572
    Epoch [70/100]	 Loss: 0.2641189283338086	 Accuracy: 0.9073275862068966
    Valid [70/100]	 val_Loss: 0.34138805270195005	 val_Accuracy: 0.8696428571428572
    Epoch [75/100]	 Loss: 0.25804773435510436	 Accuracy: 0.9110991379310345
    Valid [75/100]	 val_Loss: 0.3391507645448049	 val_Accuracy: 0.8696428571428572
    Epoch [80/100]	 Loss: 0.2523774741024807	 Accuracy: 0.9146012931034483
    Valid [80/100]	 val_Loss: 0.33722149829069775	 val_Accuracy: 0.8686011904761906
    Epoch [85/100]	 Loss: 0.24705542264313535	 Accuracy: 0.9181034482758621
    Valid [85/100]	 val_Loss: 0.3355524569749832	 val_Accuracy: 0.8686011904761906
    Epoch [90/100]	 Loss: 0.2420387501860487	 Accuracy: 0.9183728448275862
    Valid [90/100]	 val_Loss: 0.33410543004671733	 val_Accuracy: 0.8706845238095239
    Epoch [95/100]	 Loss: 0.23729218927950696	 Accuracy: 0.9205280172413793
    Valid [95/100]	 val_Loss: 0.33284951547781627	 val_Accuracy: 0.8727678571428572



```python
print("best epochs: ", best_epoch+1)
model.load_state_dict(torch.load(pre_model_path))
print("best model: ", pre_model_path)
```

    best epochs:  100
    best model:  ./models/50%/best-parameters.pt



```python
# final testing
test_loss_epoch, test_accuracy_epoch = test(
    args, arr_test_loader, model, criterion, optimizer
)
print(
    f"[FINAL]\t Loss: {test_loss_epoch / len(test_loader)}\t Accuracy: {test_accuracy_epoch / len(test_loader)}"
)
```

    [FINAL]	 Loss: 0.3631205373754104	 Accuracy: 0.8489583333333334



```python
t_1, t_2, t_3 = 0,0,0
v_1, v_2, v_3 = 0,0,0
ts_1, ts_2, ts_3 = 0,0,0

for i in train_y:
    if i == 0:
        t_1+=1
        
    elif i == 1:
        t_2+=1
        
    elif i == 2:
        t_3+=1
        
    else:
        raise Exception
        
for i in val_y:
    if i == 0:
        v_1+=1
        
    elif i == 1:
        v_2+=1
        
    elif i == 2:
        v_3+=1
        
    else:
        raise Exception
        
for i in test_y:
    if i == 0:
        ts_1+=1
        
    elif i == 1:
        ts_2+=1
        
    elif i == 2:
        ts_3+=1
        
    else:
        raise Exception
```


```python
print(t_1, t_2, t_3, 'total={}'.format(t_1+t_2+t_3))
print(v_1, v_2, v_3, 'total={}'.format(v_1+v_2+v_3))
print(ts_1, ts_2, ts_3, 'total={}'.format(ts_1+ts_2+ts_3))
```

    1264 1214 1218 total=3696
    276 326 322 total=924
    1876 770 1136 total=3782

